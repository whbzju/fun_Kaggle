{
 "metadata": {
  "name": "",
  "signature": "sha256:0a9d134f61492b73fd4b7e29ef57f06f1529870fac43b123d0bc227294f20779"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('train.csv');df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>query</th>\n",
        "      <th>product_title</th>\n",
        "      <th>product_description</th>\n",
        "      <th>median_relevance</th>\n",
        "      <th>relevance_variance</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> bridal shower decorations</td>\n",
        "      <td>       Accent Pillow with Heart Design - Red/Black</td>\n",
        "      <td> Red satin accent pillow embroidered with a hea...</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td>      led christmas lights</td>\n",
        "      <td> Set of 10 Battery Operated Multi LED Train Chr...</td>\n",
        "      <td> Set of 10 Battery Operated Train Christmas Lig...</td>\n",
        "      <td> 4</td>\n",
        "      <td> 0.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 4</td>\n",
        "      <td>                 projector</td>\n",
        "      <td>        ViewSonic Pro8200 DLP Multimedia Projector</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> 4</td>\n",
        "      <td> 0.471</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 5</td>\n",
        "      <td>                 wine rack</td>\n",
        "      <td> Concept Housewares WR-44526 Solid-Wood Ceiling...</td>\n",
        "      <td> Like a silent and sturdy tree, the Southern En...</td>\n",
        "      <td> 4</td>\n",
        "      <td> 0.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 7</td>\n",
        "      <td>                light bulb</td>\n",
        "      <td> Wintergreen Lighting Christmas LED Light Bulb ...</td>\n",
        "      <td> WTGR1011\\nFeatures\\nNickel base, 60,000 averag...</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.471</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "   id                      query  \\\n",
        "0   1  bridal shower decorations   \n",
        "1   2       led christmas lights   \n",
        "2   4                  projector   \n",
        "3   5                  wine rack   \n",
        "4   7                 light bulb   \n",
        "\n",
        "                                       product_title  \\\n",
        "0        Accent Pillow with Heart Design - Red/Black   \n",
        "1  Set of 10 Battery Operated Multi LED Train Chr...   \n",
        "2         ViewSonic Pro8200 DLP Multimedia Projector   \n",
        "3  Concept Housewares WR-44526 Solid-Wood Ceiling...   \n",
        "4  Wintergreen Lighting Christmas LED Light Bulb ...   \n",
        "\n",
        "                                 product_description  median_relevance  \\\n",
        "0  Red satin accent pillow embroidered with a hea...                 1   \n",
        "1  Set of 10 Battery Operated Train Christmas Lig...                 4   \n",
        "2                                                NaN                 4   \n",
        "3  Like a silent and sturdy tree, the Southern En...                 4   \n",
        "4  WTGR1011\\nFeatures\\nNickel base, 60,000 averag...                 2   \n",
        "\n",
        "   relevance_variance  \n",
        "0               0.000  \n",
        "1               0.000  \n",
        "2               0.471  \n",
        "3               0.000  \n",
        "4               0.471  "
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = pd.read_csv('test.csv');df_test.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>query</th>\n",
        "      <th>product_title</th>\n",
        "      <th>product_description</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  3</td>\n",
        "      <td>               electric griddle</td>\n",
        "      <td>                   Star-Max 48 in Electric Griddle</td>\n",
        "      <td>                                               NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  6</td>\n",
        "      <td>          phillips coffee maker</td>\n",
        "      <td> Philips SENSEO HD7810 WHITE Single Serve Pod C...</td>\n",
        "      <td>                                               NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  9</td>\n",
        "      <td>            san francisco 49ers</td>\n",
        "      <td>                    2013 San Francisco 49ers Clock</td>\n",
        "      <td> A 2013 San Francisco 49ers clock is the ultima...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 11</td>\n",
        "      <td>                 aveeno shampoo</td>\n",
        "      <td>               AVEENO       10.5FLOZ NRSH SHINE SH</td>\n",
        "      <td> Water, Ammonium Lauryl Sulfate, Dimethicone, S...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 12</td>\n",
        "      <td> flea and tick control for dogs</td>\n",
        "      <td> Merial Frontline Plus Flea and Tick Control fo...</td>\n",
        "      <td>                                               NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "   id                           query  \\\n",
        "0   3                electric griddle   \n",
        "1   6           phillips coffee maker   \n",
        "2   9             san francisco 49ers   \n",
        "3  11                  aveeno shampoo   \n",
        "4  12  flea and tick control for dogs   \n",
        "\n",
        "                                       product_title  \\\n",
        "0                    Star-Max 48 in Electric Griddle   \n",
        "1  Philips SENSEO HD7810 WHITE Single Serve Pod C...   \n",
        "2                     2013 San Francisco 49ers Clock   \n",
        "3                AVEENO       10.5FLOZ NRSH SHINE SH   \n",
        "4  Merial Frontline Plus Flea and Tick Control fo...   \n",
        "\n",
        "                                 product_description  \n",
        "0                                                NaN  \n",
        "1                                                NaN  \n",
        "2  A 2013 San Francisco 49ers clock is the ultima...  \n",
        "3  Water, Ammonium Lauryl Sulfate, Dimethicone, S...  \n",
        "4                                                NaN  "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(df['query'].unique())\n",
      "df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "261\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "(10158, 6)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.groupby('query').size()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "query\n",
        "16 gb memory card              64\n",
        "8 ounce mason jars             10\n",
        "acoustic guitar clamp          30\n",
        "adidas fragance                28\n",
        "adidas pants                   24\n",
        "an extremely goofy movie       21\n",
        "anime necklace                 15\n",
        "apple iphone 32 gb otterbox    18\n",
        "aqua shoes                     47\n",
        "aroma diffuser                 54\n",
        "assassinss creed               54\n",
        "aveeno shampoo                 51\n",
        "barbie                         45\n",
        "baseball cleats                29\n",
        "baseball photo frame           28\n",
        "...\n",
        "wii                           51\n",
        "wii gamepad                   35\n",
        "wii microphone                31\n",
        "wine rack                     53\n",
        "wired xbox 360 controller     30\n",
        "wireless mouse               113\n",
        "workout clothes for women     48\n",
        "wreck it ralph                53\n",
        "yankee candle                 29\n",
        "yankees                       36\n",
        "yellow dress                  41\n",
        "yoga mat                      33\n",
        "yoga pants                    42\n",
        "zippo                         35\n",
        "zippo hand warmer             30\n",
        "Length: 261, dtype: int64"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.groupby('query')['median_relevance'].mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "query\n",
        "16 gb memory card              3.812500\n",
        "8 ounce mason jars             2.500000\n",
        "acoustic guitar clamp          2.866667\n",
        "adidas fragance                3.678571\n",
        "adidas pants                   2.916667\n",
        "an extremely goofy movie       3.000000\n",
        "anime necklace                 2.533333\n",
        "apple iphone 32 gb otterbox    2.611111\n",
        "aqua shoes                     2.851064\n",
        "aroma diffuser                 3.185185\n",
        "assassinss creed               3.481481\n",
        "aveeno shampoo                 3.549020\n",
        "barbie                         3.244444\n",
        "baseball cleats                3.034483\n",
        "baseball photo frame           3.142857\n",
        "...\n",
        "wii                          2.921569\n",
        "wii gamepad                  2.742857\n",
        "wii microphone               2.870968\n",
        "wine rack                    3.679245\n",
        "wired xbox 360 controller    3.466667\n",
        "wireless mouse               3.796460\n",
        "workout clothes for women    3.041667\n",
        "wreck it ralph               3.415094\n",
        "yankee candle                3.758621\n",
        "yankees                      2.555556\n",
        "yellow dress                 3.170732\n",
        "yoga mat                     3.363636\n",
        "yoga pants                   3.523810\n",
        "zippo                        3.571429\n",
        "zippo hand warmer            3.433333\n",
        "Name: median_relevance, Length: 261, dtype: float64"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.groupby('query')['median_relevance'].std()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "query\n",
        "16 gb memory card              0.613990\n",
        "8 ounce mason jars             0.849837\n",
        "acoustic guitar clamp          1.166585\n",
        "adidas fragance                0.611832\n",
        "adidas pants                   1.138904\n",
        "an extremely goofy movie       1.264911\n",
        "anime necklace                 0.743223\n",
        "apple iphone 32 gb otterbox    1.092159\n",
        "aqua shoes                     0.999537\n",
        "aroma diffuser                 1.229727\n",
        "assassinss creed               0.693385\n",
        "aveeno shampoo                 0.832195\n",
        "barbie                         0.773292\n",
        "baseball cleats                1.267246\n",
        "baseball photo frame           0.705234\n",
        "...\n",
        "wii                          0.844823\n",
        "wii gamepad                  0.950011\n",
        "wii microphone               1.117794\n",
        "wine rack                    0.672927\n",
        "wired xbox 360 controller    0.860366\n",
        "wireless mouse               0.502836\n",
        "workout clothes for women    1.009705\n",
        "wreck it ralph               0.928894\n",
        "yankee candle                0.510964\n",
        "yankees                      1.026630\n",
        "yellow dress                 0.972174\n",
        "yoga mat                     0.962360\n",
        "yoga pants                   0.740405\n",
        "zippo                        0.814779\n",
        "zippo hand warmer            0.935261\n",
        "Name: median_relevance, Length: 261, dtype: float64"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "measurements = [\n",
      "...     {'city': 'Dubai', 'temperature': 33.},\n",
      "...     {'city': 'London', 'temperature': 12.},\n",
      "...     {'city': 'San Fransisco', 'temperature': 18.},\n",
      "... ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "...     'This is the first document.',\n",
      "...     'This is the second second document.',\n",
      "...     'And the third one.',\n",
      "...     'Is this the first document?',\n",
      "... ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.fit_transform(corpus);X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "<4x9 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 19 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      ">>> analyze = vectorizer.build_analyzer()\n",
      ">>> analyze(\"This is a text document to analyze.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[u'this', u'is', u'text', u'document', u'to', u'analyze']"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      ">>> vectorizer.get_feature_names()         "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "[u'and',\n",
        " u'document',\n",
        " u'first',\n",
        " u'is',\n",
        " u'one',\n",
        " u'second',\n",
        " u'the',\n",
        " u'third',\n",
        " u'this']"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## benchmark"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import re\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "train = pd.read_csv(\"train.csv\").fillna(\"\")\n",
      "test  = pd.read_csv(\"test.csv\").fillna(\"\")\n",
      "\n",
      "class FeatureMapper:\n",
      "    def __init__(self, features):\n",
      "        self.features = features\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        for feature_name, column_name, extractor in self.features:\n",
      "            extractor.fit(X[column_name], y)\n",
      "\n",
      "    def transform(self, X):\n",
      "        extracted = []\n",
      "        for feature_name, column_name, extractor in self.features:\n",
      "            fea = extractor.transform(X[column_name])\n",
      "            if hasattr(fea, \"toarray\"):\n",
      "                extracted.append(fea.toarray())\n",
      "            else:\n",
      "                extracted.append(fea)\n",
      "        if len(extracted) > 1:\n",
      "            return np.concatenate(extracted, axis=1)\n",
      "        else: \n",
      "            return extracted[0]\n",
      "\n",
      "    def fit_transform(self, X, y=None):\n",
      "        extracted = []\n",
      "        for feature_name, column_name, extractor in self.features:\n",
      "            fea = extractor.fit_transform(X[column_name], y)\n",
      "            if hasattr(fea, \"toarray\"):\n",
      "                extracted.append(fea.toarray())\n",
      "            else:\n",
      "                extracted.append(fea)\n",
      "        if len(extracted) > 1:\n",
      "            return np.concatenate(extracted, axis=1)\n",
      "        else: \n",
      "            return extracted[0]\n",
      "\n",
      "def identity(x):\n",
      "    return x\n",
      "\n",
      "class SimpleTransform(BaseEstimator):\n",
      "    def __init__(self, transformer=identity):\n",
      "        self.transformer = transformer\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def fit_transform(self, X, y=None):\n",
      "        return self.transform(X)\n",
      "\n",
      "    def transform(self, X, y=None):\n",
      "        return np.array([self.transformer(x) for x in X], ndmin=2).T\n",
      "\n",
      "#                          Feature Set Name            Data Frame Column              Transformer\n",
      "features = FeatureMapper([('QueryBagOfWords',          'query',                       CountVectorizer(max_features=200)),\n",
      "                          ('TitleBagOfWords',          'product_title',               CountVectorizer(max_features=200)),\n",
      "                          ('DescriptionBagOfWords',    'product_description',         CountVectorizer(max_features=200)),\n",
      "                          ('QueryTokensInTitle',       'query_tokens_in_title',       SimpleTransform()),\n",
      "                          ('QueryTokensInDescription', 'query_tokens_in_description', SimpleTransform())])\n",
      "\n",
      "def extract_features(data):\n",
      "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
      "    data[\"query_tokens_in_title\"] = 0.0\n",
      "    data[\"query_tokens_in_description\"] = 0.0\n",
      "    for i, row in data.iterrows():\n",
      "        query = set(x.lower() for x in token_pattern.findall(row[\"query\"]))\n",
      "        title = set(x.lower() for x in token_pattern.findall(row[\"product_title\"]))\n",
      "        description = set(x.lower() for x in token_pattern.findall(row[\"product_description\"]))\n",
      "        if len(title) > 0:\n",
      "            data.set_value(i, \"query_tokens_in_title\", len(query.intersection(title))/len(title))\n",
      "        if len(description) > 0:\n",
      "            data.set_value(i, \"query_tokens_in_description\", len(query.intersection(description))/len(description))\n",
      "\n",
      "extract_features(train)\n",
      "extract_features(test)\n",
      "\n",
      "pipeline = Pipeline([(\"extract_features\", features),\n",
      "                     (\"classify\", RandomForestClassifier(n_estimators=200,\n",
      "                                                         n_jobs=1,\n",
      "                                                         min_samples_split=2,\n",
      "                                                         random_state=1))])\n",
      "\n",
      "pipeline.fit(train, train[\"median_relevance\"])\n",
      "\n",
      "predictions = pipeline.predict(test)\n",
      "\n",
      "submission = pd.DataFrame({\"id\": test[\"id\"], \"prediction\": predictions})\n",
      "submission.to_csv(\"python_benchmark.csv\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = FeatureMapper([('QueryBagOfWords',          'query',                       CountVectorizer(max_features=200))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = FeatureMapper([('QueryBagOfWords',          'query',                       CountVectorizer(max_features=200)),\n",
      "                          ('TitleBagOfWords',          'product_title',               CountVectorizer(max_features=200)),\n",
      "                          ('DescriptionBagOfWords',    'product_description',         CountVectorizer(max_features=200)),\n",
      "                          ('QueryTokensInTitle',       'query_tokens_in_title',       SimpleTransform()),\n",
      "                          ('QueryTokensInDescription', 'query_tokens_in_description', SimpleTransform())])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline = Pipeline([(\"extract_features\", features)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = features.transform(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " ..., \n",
        " [ 0.  0.  0. ...,  3.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## new benchmark"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\"\"\"\n",
      "Beating the Benchmark \n",
      "Search Results Relevance @ Kaggle\n",
      "__author__ : Abhishek\n",
      "\n",
      "\"\"\"\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import decomposition, pipeline, metrics, grid_search\n",
      "from nltk.stem.porter import *\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "from bs4 import BeautifulSoup\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction import text\n",
      "# array declarations\n",
      "sw=[]\n",
      "s_data = []\n",
      "s_labels = []\n",
      "t_data = []\n",
      "t_labels = []\n",
      "#stopwords tweak - more overhead\n",
      "stop_words = ['http','www','img','border','0','1','2','3','4','5','6','7','8','9']\n",
      "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
      "for stw in stop_words:\n",
      "    sw.append(\"q\"+stw)\n",
      "    sw.append(\"z\"+stw)\n",
      "stop_words = text.ENGLISH_STOP_WORDS.union(sw)\n",
      "\n",
      "\n",
      "# The following 3 functions have been taken from Ben Hamner's github repository\n",
      "# https://github.com/benhamner/Metrics\n",
      "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
      "    \"\"\"\n",
      "    Returns the confusion matrix between rater's ratings\n",
      "    \"\"\"\n",
      "    assert(len(rater_a) == len(rater_b))\n",
      "    if min_rating is None:\n",
      "        min_rating = min(rater_a + rater_b)\n",
      "    if max_rating is None:\n",
      "        max_rating = max(rater_a + rater_b)\n",
      "    num_ratings = int(max_rating - min_rating + 1)\n",
      "    conf_mat = [[0 for i in range(num_ratings)]\n",
      "                for j in range(num_ratings)]\n",
      "    for a, b in zip(rater_a, rater_b):\n",
      "        conf_mat[a - min_rating][b - min_rating] += 1\n",
      "    return conf_mat\n",
      "\n",
      "\n",
      "def histogram(ratings, min_rating=None, max_rating=None):\n",
      "    \"\"\"\n",
      "    Returns the counts of each type of rating that a rater made\n",
      "    \"\"\"\n",
      "    if min_rating is None:\n",
      "        min_rating = min(ratings)\n",
      "    if max_rating is None:\n",
      "        max_rating = max(ratings)\n",
      "    num_ratings = int(max_rating - min_rating + 1)\n",
      "    hist_ratings = [0 for x in range(num_ratings)]\n",
      "    for r in ratings:\n",
      "        hist_ratings[r - min_rating] += 1\n",
      "    return hist_ratings\n",
      "\n",
      "\n",
      "def quadratic_weighted_kappa(y, y_pred):\n",
      "    \"\"\"\n",
      "    Calculates the quadratic weighted kappa\n",
      "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
      "    value, which is a measure of inter-rater agreement between two raters\n",
      "    that provide discrete numeric ratings.  Potential values range from -1\n",
      "    (representing complete disagreement) to 1 (representing complete\n",
      "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
      "    chance.\n",
      "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
      "    each correspond to a list of integer ratings.  These lists must have the\n",
      "    same length.\n",
      "    The ratings should be integers, and it is assumed that they contain\n",
      "    the complete range of possible ratings.\n",
      "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
      "    is the minimum possible rating, and max_rating is the maximum possible\n",
      "    rating\n",
      "    \"\"\"\n",
      "    rater_a = y\n",
      "    rater_b = y_pred\n",
      "    min_rating=None\n",
      "    max_rating=None\n",
      "    rater_a = np.array(rater_a, dtype=int)\n",
      "    rater_b = np.array(rater_b, dtype=int)\n",
      "    assert(len(rater_a) == len(rater_b))\n",
      "    if min_rating is None:\n",
      "        min_rating = min(min(rater_a), min(rater_b))\n",
      "    if max_rating is None:\n",
      "        max_rating = max(max(rater_a), max(rater_b))\n",
      "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
      "                                min_rating, max_rating)\n",
      "    num_ratings = len(conf_mat)\n",
      "    num_scored_items = float(len(rater_a))\n",
      "\n",
      "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
      "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
      "\n",
      "    numerator = 0.0\n",
      "    denominator = 0.0\n",
      "\n",
      "    for i in range(num_ratings):\n",
      "        for j in range(num_ratings):\n",
      "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
      "                              / num_scored_items)\n",
      "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
      "            numerator += d * conf_mat[i][j] / num_scored_items\n",
      "            denominator += d * expected_count / num_scored_items\n",
      "\n",
      "    return (1.0 - numerator / denominator)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "\n",
      "    # Load the training file\n",
      "    train = pd.read_csv('train.csv')\n",
      "    test = pd.read_csv('test.csv')\n",
      "    \n",
      "    # we dont need ID columns\n",
      "    idx = test.id.values.astype(int)\n",
      "    train = train.drop('id', axis=1)\n",
      "    test = test.drop('id', axis=1)\n",
      "    \n",
      "    # create labels. drop useless columns\n",
      "    y = train.median_relevance.values\n",
      "    train = train.drop(['median_relevance', 'relevance_variance'], axis=1)\n",
      "    \n",
      "    # do some lambda magic on text columns\n",
      "    traindata = list(train.apply(lambda x:'%s %s' % (x['query'],x['product_title']),axis=1))\n",
      "    testdata = list(test.apply(lambda x:'%s %s' % (x['query'],x['product_title']),axis=1))\n",
      "    \n",
      "    # the infamous tfidf vectorizer (Do you remember this one?)\n",
      "    tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
      "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "            ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
      "            stop_words = 'english')\n",
      "    \n",
      "    # Fit TFIDF\n",
      "    tfv.fit(traindata)\n",
      "    X =  tfv.transform(traindata) \n",
      "    X_test = tfv.transform(testdata)\n",
      "    \n",
      "    # Initialize SVD\n",
      "    svd = TruncatedSVD()\n",
      "    \n",
      "    # Initialize the standard scaler \n",
      "    scl = StandardScaler()\n",
      "    \n",
      "    # We will use SVM here..\n",
      "    svm_model = SVC()\n",
      "    \n",
      "    # Create the pipeline \n",
      "    clf = pipeline.Pipeline([('svd', svd),\n",
      "    \t\t\t\t\t\t ('scl', scl),\n",
      "                    \t     ('svm', svm_model)])\n",
      "    \n",
      "    # Create a parameter grid to search for best parameters for everything in the pipeline\n",
      "    param_grid = {'svd__n_components' : [400],\n",
      "                  'svm__C': [10]}\n",
      "    \n",
      "    # Kappa Scorer \n",
      "    kappa_scorer = metrics.make_scorer(quadratic_weighted_kappa, greater_is_better = True)\n",
      "    \n",
      "    # Initialize Grid Search Model\n",
      "    model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n",
      "                                     verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
      "                                     \n",
      "    # Fit Grid Search Model\n",
      "    model.fit(X, y)\n",
      "    print(\"Best score: %0.3f\" % model.best_score_)\n",
      "    print(\"Best parameters set:\")\n",
      "    best_parameters = model.best_estimator_.get_params()\n",
      "    for param_name in sorted(param_grid.keys()):\n",
      "    \tprint(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
      "    \n",
      "    # Get best model\n",
      "    best_model = model.best_estimator_\n",
      "    \n",
      "    # Fit model with best parameters optimized for quadratic_weighted_kappa\n",
      "    best_model.fit(X,y)\n",
      "    preds = best_model.predict(X_test)\n",
      "    \n",
      "    #load data\n",
      "    train = pd.read_csv(\"train.csv\").fillna(\"\")\n",
      "    test  = pd.read_csv(\"test.csv\").fillna(\"\")\n",
      "    \n",
      "    #remove html, remove non text or numeric, make query and title unique features for counts using prefix (accounted for in stopwords tweak)\n",
      "    stemmer = PorterStemmer()\n",
      "    ## Stemming functionality\n",
      "    class stemmerUtility(object):\n",
      "        \"\"\"Stemming functionality\"\"\"\n",
      "        @staticmethod\n",
      "        def stemPorter(review_text):\n",
      "            porter = PorterStemmer()\n",
      "            preprocessed_docs = []\n",
      "            for doc in review_text:\n",
      "                final_doc = []\n",
      "                for word in doc:\n",
      "                    final_doc.append(porter.stem(word))\n",
      "                    #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n",
      "                preprocessed_docs.append(final_doc)\n",
      "            return preprocessed_docs\n",
      "    \n",
      "    \n",
      "    for i in range(len(train.id)):\n",
      "        s=(\" \").join([\"q\"+ z for z in BeautifulSoup(train[\"query\"][i]).get_text(\" \").split(\" \")]) + \" \" + (\" \").join([\"z\"+ z for z in BeautifulSoup(train.product_title[i]).get_text(\" \").split(\" \")]) + \" \" + BeautifulSoup(train.product_description[i]).get_text(\" \")\n",
      "        s=re.sub(\"[^a-zA-Z0-9]\",\" \", s)\n",
      "        s= (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
      "        s_data.append(s)\n",
      "        s_labels.append(str(train[\"median_relevance\"][i]))\n",
      "    for i in range(len(test.id)):\n",
      "        s=(\" \").join([\"q\"+ z for z in BeautifulSoup(test[\"query\"][i]).get_text().split(\" \")]) + \" \" + (\" \").join([\"z\"+ z for z in BeautifulSoup(test.product_title[i]).get_text().split(\" \")]) + \" \" + BeautifulSoup(test.product_description[i]).get_text()\n",
      "        s=re.sub(\"[^a-zA-Z0-9]\",\" \", s)\n",
      "        s= (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
      "        t_data.append(s)\n",
      "    #create sklearn pipeline, fit all, and predit test data\n",
      "    clf = Pipeline([('v',TfidfVectorizer(min_df=5, max_df=500, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 2), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = 'english')), \n",
      "    ('svd', TruncatedSVD(n_components=200, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)), \n",
      "    ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), \n",
      "    ('svm', SVC(C=10.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None))])\n",
      "    clf.fit(s_data, s_labels)\n",
      "    t_labels = clf.predict(t_data)\n",
      "    \n",
      "    import math\n",
      "    p3 = []\n",
      "    for i in range(len(preds)):\n",
      "        x = (int(t_labels[i]) + preds[i])/2\n",
      "        x = math.floor(x)\n",
      "        p3.append(int(x))\n",
      "        \n",
      "        \n",
      "    \n",
      "    # p3 = (t_labels + preds)/2\n",
      "    # p3 = p3.apply(lambda x:math.floor(x))\n",
      "    # p3 = p3.apply(lambda x:int(x))\n",
      "    \n",
      "    # preds12 = \n",
      "\n",
      "    # Create your first submission file\n",
      "    submission = pd.DataFrame({\"id\": idx, \"prediction\": p3})\n",
      "    submission.to_csv(\"beating_the_benchmark_yet_again.csv\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}